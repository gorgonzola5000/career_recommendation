{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    logfile = f\"log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%m-%Y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    \n",
    "    return logging\n",
    "\n",
    "def create_file(file, logging):\n",
    "    logging.info(\"Creating daily csv file...\")\n",
    "    header = ['date_time', 'search_keyword', 'search_count', 'job_id', 'update_time', 'job_title', 'company', 'location', 'job_description']\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        logging.info(f\"{file} created\")\n",
    "\n",
    "def login(logging):\n",
    "    url_login = \"https://pl.linkedin.com/\"\n",
    "    load_dotenv()\n",
    "    # structure of .env:\n",
    "    # LINKEDIN_USERNAME=email@gmail.com\n",
    "    # LINKEDIN_PASSWORD=password\n",
    "\n",
    "    LINKEDIN_USERNAME = os.getenv('LINKEDIN_USERNAME')\n",
    "    LINKEDIN_PASSWORD = os.getenv('LINKEDIN_PASSWORD')\n",
    "\n",
    "    # start chrome\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    # login to LinkedIn\n",
    "    logging.info(f\"Logging in to LinkedIn as {LINKEDIN_USERNAME}...\")\n",
    "    wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n",
    "    wd.get(url_login)\n",
    "    wd.find_element(By.ID, \"session_key\").send_keys(LINKEDIN_USERNAME)\n",
    "    wd.find_element(By.ID, \"session_password\").send_keys(LINKEDIN_PASSWORD)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    wd.find_element(By.XPATH, \"//button[@type = 'submit']\").click()\n",
    "    time.sleep(15)\n",
    "    logging.info(\"Log in complete. Scraping data...\")\n",
    "\n",
    "    return wd\n",
    "def page_search(wd, search_keyword, search_page, search_count, file, logging):\n",
    "    # wait time for events in seconds\n",
    "    page_wait = 20 + random.uniform(-5,5)\n",
    "    click_wait = 5\n",
    "    attempts = 3 # times to retry when trouble with scraping element\n",
    "    list_jobs = []\n",
    "    url_search = f\"https://www.linkedin.com/jobs/search/?keywords={search_keyword}&start={search_page}\"\n",
    "    wd.get(url_search)\n",
    "    time.sleep(page_wait)\n",
    "    try:\n",
    "        search_count = wd.find_element(By.XPATH, '//small').text\n",
    "        search_count = search_count.split(\" \")\n",
    "        # numbers in range <1 000; 999 999> are split into a list of 3 strings eg. \"1 234 wyniki\" => [\"1\", \"234\", \"wyniki\"] \n",
    "        # numbers in range <1; 999> are split into a list of 2 strings\n",
    "        # we want to ditch the word \"wyniki\"/\"wynikÃ³w\"\n",
    "        if len(search_count)>2:\n",
    "            search_count = search_count[0]+search_count[1]\n",
    "        else:\n",
    "            search_count = search_count[0]\n",
    "        search_count = int(search_count)\n",
    "    except:\n",
    "        search_count = 999999999\n",
    "\n",
    "    logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")\n",
    "    # get all the job_id's for xpath for current page to click each element\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            search_results = wd.find_elements(By.XPATH, \"//ul[@class='scaffold-layout__list-container']/li\")\n",
    "            result_ids = [result.get_attribute('data-occludable-job-id') for result in search_results if result.get_attribute('data-occludable-job-id') != '']\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(click_wait)\n",
    "    for job_id in result_ids:\n",
    "        random_wait = random.uniform(-1,1)\n",
    "        click_wait=click_wait+random_wait\n",
    "        time.sleep(click_wait)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                wd.find_element(By.XPATH, f\"//ul[@class='scaffold-layout__list-container']/li[@data-occludable-job-id='{job_id}']\").click()\n",
    "                time.sleep(0.2)\n",
    "            except:\n",
    "                time.sleep(click_wait)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                job_title = wd.find_element(By.XPATH, '//h2[@class=\"t-24 t-bold jobs-unified-top-card__job-title\"]').text\n",
    "                break\n",
    "            except:\n",
    "                job_title = ''\n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                company = wd.find_element(By.XPATH, '//a[contains(@class, \"ember-view t-black t-normal\")]').text\n",
    "                location = wd.find_element(By.XPATH, '//span[contains(@class, \"jobs-unified-top-card__bullet\")]').text\n",
    "                break\n",
    "            except:\n",
    "                company = ''\n",
    "                location = ''\n",
    "                time.sleep(click_wait)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                update_time = wd.find_element(By.XPATH, '//span[contains(@class, \"jobs-unified-top-card__posted-date\")]').text\n",
    "                break\n",
    "            except: \n",
    "                update_time = ''\n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                job_description = wd.find_element(By.XPATH, '//div[@id=\"job-details\"]/span')\n",
    "                job_description = job_description.text\n",
    "                break\n",
    "            except:\n",
    "                job_description = ''\n",
    "                time.sleep(click_wait)\n",
    "\n",
    "        # append (a) line to file\n",
    "        date_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        search_keyword = search_keyword.replace(\"%20\", \" \")\n",
    "        list_job = [date_time, search_keyword, search_count, job_id, update_time, job_title, company, location, job_description]\n",
    "        for element in range(len(list_job)):\n",
    "            if \",\" in str(list_job[element]):\n",
    "                list_job[element]=list_job[element].replace(\",\", \" \")\n",
    "            if \"\\n\" in str(list_job[element]):\n",
    "                list_job[element]=list_job[element].replace(\"\\n\", \" \")\n",
    "        list_jobs.append(list_job)\n",
    "        list_job = []\n",
    "    with open(file, \"a\", newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(list_jobs)\n",
    "    \n",
    "    list_jobs = []\n",
    "    \n",
    "    logging.info(f\"Page {round(search_page/25) + 1} of {round(search_count/25)} loaded for {search_keyword}\")\n",
    "    search_page += 25\n",
    "\n",
    "    return search_page, search_count, url_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evera\\AppData\\Local\\Temp\\ipykernel_9916\\1815958588.py:43: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "# create logging file\n",
    "logging = create_logfile()\n",
    "\n",
    "# create csv file\n",
    "date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "file = f\"output/{date}.csv\"\n",
    "create_file(file, logging)\n",
    "\n",
    "# login to linkedin and assign webdriver to variable\n",
    "wd = login(logging)\n",
    "\n",
    "# URL search terms focusing on what type of skills are required for Data Analyst & Data Scientist\n",
    "search_keywords = ['Data Analyst', 'Data Scientist', 'Data Engineer']\n",
    "\n",
    "# Counting Exceptions\n",
    "exception_first = 0\n",
    "exception_second = 0\n",
    "\n",
    "for search_keyword in search_keywords:\n",
    "    search_keyword = search_keyword.lower().replace(\" \", \"%20\")\n",
    "\n",
    "# Loop through each page and write results to csv\n",
    "    search_page = 0\n",
    "    search_count = 1\n",
    "    while (search_page < search_count) and (search_page != 1000):\n",
    "        try:\n",
    "            search_page, search_count, url_search = page_search(wd, search_keyword, search_page, search_count, file, logging)\n",
    "        except Exception as e:\n",
    "            logging.error(f'(1) FIRST exception for {search_keyword} on {search_page} of {search_count}, retrying...')\n",
    "            logging.error(f'Current URL: {url_search}')\n",
    "            logging.error(e)\n",
    "            logging.exception('Traceback ->')\n",
    "            exception_first += 1\n",
    "            time.sleep(5) \n",
    "            try:\n",
    "                search_page, search_count, url_search = page_search(wd, search_keyword, search_page, search_count, file, logging)\n",
    "                logging.warning(f'Solved Exception for {search_keyword} on {search_page} of {search_count}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'(2) SECOND exception remains for {search_keyword}. Skipping to next page...')\n",
    "                logging.error(f'Current URL: {url_search}')\n",
    "                logging.error(e)\n",
    "                logging.exception('Traceback ->')\n",
    "                search_page += 25 # skip to next page to avoid entry\n",
    "                exception_second += 1\n",
    "                logging.error(f'Skipping to next page for {search_keyword}, on {search_page} of {search_count}...')\n",
    "\n",
    "\n",
    "logging.info(f'LinkedIn data scraping complete with {exception_first} first and {exception_second} second exceptions')\n",
    "logging.info(f'Regard all further alarms...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "823cc90b37ed9db33e54be9e81b40076d2dd102c833bf5e516ed31deb28794da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
