{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H-%M-%S')\n",
    "    logfile = f\"log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H-%M-%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    \n",
    "    return logging\n",
    "def create_file(file, logging):\n",
    "    # delete existing file if re-running\n",
    "    logging.info(\"Checking if current daily csv exists...\")\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{file} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{file} ain't exist\")\n",
    "    \n",
    "    # create file and add header\n",
    "    logging.info(\"Creating daily csv file...\")\n",
    "    header = ['date_time', 'search_keyword', 'search_count', 'job_id', 'job_title', 'company', 'location', 'remote', 'update_time', 'applicants', 'job_pay', 'job_time', 'job_position', 'company_size', 'company_industry', 'job_details']\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        logging.info(f\"{file} created\")\n",
    "def login(logging):\n",
    "    url_login = \"https://pl.linkedin.com/\"\n",
    "\n",
    "    # pulls login information from file called '.env' \n",
    "    # this file added to .gitignore so login details not shared\n",
    "    load_dotenv()\n",
    "    # .env file is of structure:\n",
    "    # LINKEDIN_USERNAME=email@gmail.com\n",
    "    # LINKEDIN_PASSWORD=password\n",
    "\n",
    "    LINKEDIN_USERNAME = os.getenv('LINKEDIN_USERNAME')\n",
    "    LINKEDIN_PASSWORD = os.getenv('LINKEDIN_PASSWORD')\n",
    "\n",
    "    # start chrome\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "\n",
    "    # login to LinkedIn\n",
    "    logging.info(f\"Logging in to LinkedIn as {LINKEDIN_USERNAME}...\")\n",
    "    wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n",
    "    wd.get(url_login)\n",
    "    wd.find_element(By.ID, \"session_key\").send_keys(LINKEDIN_USERNAME)\n",
    "    wd.find_element(By.ID, \"session_password\").send_keys(LINKEDIN_PASSWORD)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    wd.find_element(By.XPATH, \"//button[@type = 'submit']\").click()\n",
    "    time.sleep(20)\n",
    "    # random confirm acount information pop up that may come up\n",
    "    try: \n",
    "        wd.find_element(By.XPATH, \"//button[@class='primary-action-new']\").click()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    logging.info(\"Log in complete. Scraping data...\")\n",
    "\n",
    "    return wd\n",
    "def page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging):\n",
    "    # wait time for events in seconds\n",
    "    page_wait = 30\n",
    "    click_wait = 5\n",
    "    async_wait = 5\n",
    "    # when retrying, number of attempts\n",
    "    attempts = 3\n",
    "    # navigate to search page\n",
    "    url_search = f\"https://www.linkedin.com/jobs/search/?keywords={search_keyword}\"\n",
    "    wd.get(url_search)\n",
    "    time.sleep(page_wait) # add sleep so don't get caught\n",
    "    # find the number of results\n",
    "    search_count = wd.find_element(By.XPATH, '//small').text\n",
    "    search_count = search_count.split(\" \")\n",
    "    # numbers in range <1 000; 999 999> are split into a list of 3 strings eg. \"1 234 wyniki\" => [\"1\", \"234\", \"wyniki\"] \n",
    "    # numbers in range <1; 999> are split into a list of 2 strings\n",
    "    # we want to ditch the word \"wyniki\"/\"wyników\"\n",
    "    if len(search_count)>2:\n",
    "        search_count = search_count[0]+search_count[1]\n",
    "    else:\n",
    "        search_count = search_count[0]\n",
    "    search_count = int(search_count)\n",
    "    logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")\n",
    "    # get all the job_id's for xpath for current page to click each element\n",
    "    # running into errors with slow load (11-Aug)\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            search_results = wd.find_elements(By.XPATH, \"//ul[@class='scaffold-layout__list-container']/li\")\n",
    "            result_ids = [result.get_attribute('data-occludable-job-id') for result in search_results if result.get_attribute('data-occludable-job-id') != '']\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(click_wait) # wait a few attempts, if not throw an exception and then skip to next page\n",
    "    # cycle through each job_ids and steal the job data\n",
    "    list_jobs = [] #initate a blank list to append each page to\n",
    "    for job_id in result_ids:\n",
    "        random_wait = random.randint(-5, 5)\n",
    "        page_wait=page_wait+random_wait\n",
    "        wd.get(f\"https://www.linkedin.com/jobs/search/?currentJobId={job_id}&keywords={search_keyword}\")\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                # from analysis 3 attempts at 5 second waits gets job titles 99.99% of time (11-Aug)\n",
    "                job_title = wd.find_element(By.XPATH, '//h2[@class=\"t-24 t-bold jobs-unified-top-card__job-title\"]') # keep having issues with finding element\n",
    "                job_title = job_title.text\n",
    "                break\n",
    "            except:\n",
    "                job_title = ''\n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        # Having issues finding xpath of company (Added 11-Aug)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                #company = wd.find_element(By.XPATH, \"/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[2]/div/div[2]/div[1]/div/div[1]/div/div[1]/div[1]/div[1]/span[1]/span[1]/a\").text\n",
    "                #location = wd.find_element(By.XPATH, \"/html/body/div[6]/div[3]/div[4]/div/div/main/div/section[2]/div/div[2]/div[1]/div/div[1]/div/div[1]/div[1]/div[1]/span[1]/span[2]\").text\n",
    "                company = wd.find_element(By.XPATH, '//a[contains(@class, \"ember-view t-black t-normal\")]').text\n",
    "                location = wd.find_element(By.XPATH, '//span[contains(@class, \"jobs-unified-top-card__bullet\")]').text\n",
    "                \"\"\"\n",
    "                if len(job_top_card1) > 2: # only displays remote if selected, otherwise only 2 elements in list\n",
    "                    remote = job_top_card1[2].text\n",
    "                else:\n",
    "                    remote = ''\n",
    "                \"\"\"\n",
    "                break\n",
    "            except:\n",
    "                company = ''\n",
    "                location = ''\n",
    "                remote = ''\n",
    "                time.sleep(click_wait)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                update_time = wd.find_element(By.XPATH, '//span[contains(@class, \"jobs-unified-top-card__posted-date\")]')\n",
    "                break\n",
    "            except: \n",
    "                update_time = '' # after #attempts leave as blank and move on\n",
    "                time.sleep(click_wait)\n",
    "        # Due to (slow) ASYNCHRONOUS updates, need wait times to get job_info\n",
    "        job_time = '' # assigning as blanks as not important info and can skip if not obtained below\n",
    "        job_position = ''\n",
    "        job_pay = ''\n",
    "        #WORKS UP TO HERE\n",
    "        \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                # 1 - make sure HTML element is loaded\n",
    "                element = WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"mt5 mb2\")]/ul')))\n",
    "                # 2 - make sure text is loaded\n",
    "                try:\n",
    "                    job_info = element.text\n",
    "                    if job_info != '':\n",
    "                        # seperate job information on time requirements and position\n",
    "                        job_info = job_info.split(\" · \")\n",
    "                        if len(job_info) == 1: # only one item means its job _time\n",
    "                            job_pay = ''\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = ''\n",
    "                        elif (len(job_info) >= 2) and (\"$\" in job_info[0]): # if has money symbol then seperate\n",
    "                            job_pay = job_info[0]\n",
    "                            job_time = job_info[1]\n",
    "                            if(len(job_info)>= 3): # check if job_info is required\n",
    "                                job_position = job_info[2]\n",
    "                            else:\n",
    "                                job_position = ''\n",
    "                        else: # else condition satisifies the last condition\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = job_info[1]\n",
    "                            job_pay = ''\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(async_wait)\n",
    "                except:\n",
    "                    # error means page didn't load so try again\n",
    "                    time.sleep(async_wait)\n",
    "            except:\n",
    "                # error means page didn't load so try again\n",
    "                time.sleep(async_wait)\n",
    "        \n",
    "\n",
    "        # get company details and seperate on size and industry\n",
    "        company_size = '' # assigning as blanks as not important info and can skip if not obtained below\n",
    "        company_industry = ''\n",
    "        job_details = ''      \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                company_details = wd.find_element(By.XPATH, \"//div[@class='mt5 mb2']/div[2]\").text\n",
    "                if \" · \" in company_details:\n",
    "                    company_size = company_details.split(\" · \")[0]\n",
    "                    company_industry = company_details.split(\" · \")[1]\n",
    "                else:\n",
    "                    company_size = company_details\n",
    "                    company_industry = ''\n",
    "                job_details = wd.find_element(By.ID, \"job-details\").text.replace(\"\\n\", \" \")\n",
    "                break\n",
    "            except: \n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        # append (a) line to file\n",
    "        date_time = datetime.datetime.now().strftime(\"%d%b%Y-%H:%M:%S\")\n",
    "        search_keyword = search_keyword.replace(\"%20\", \" \")\n",
    "        #list_job = [date_time, search_keyword, search_count, job_id, job_title, company, location, remote, update_time, applicants, job_pay, job_time, job_position, company_size, company_industry, job_details]\n",
    "        list_job = [date_time, search_keyword, search_count, job_id, job_title, company, location, remote]\n",
    "        list_jobs.append(list_job)\n",
    "\n",
    "    with open(file, \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(list_jobs)\n",
    "        list_jobs = []\n",
    "    \n",
    "    logging.info(f\"Page {round(search_page/25) + 1} of {round(search_count/25)} loaded for {search_keyword}\")\n",
    "    search_page += 25\n",
    "\n",
    "    return search_page, search_count, url_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evera\\AppData\\Local\\Temp\\ipykernel_16316\\341557071.py:56: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'url_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     search_page, search_count, url_search \u001b[39m=\u001b[39m page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n\u001b[0;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m, in \u001b[0;36mpage_search\u001b[1;34m(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\u001b[0m\n\u001b[0;32m     81\u001b[0m url_search \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.linkedin.com/jobs/search/?keywords=\u001b[39m\u001b[39m{\u001b[39;00msearch_keyword\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 82\u001b[0m wd\u001b[39m.\u001b[39;49mget(url_search)\n\u001b[0;32m     83\u001b[0m time\u001b[39m.\u001b[39msleep(page_wait) \u001b[39m# add sleep so don't get caught\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\evera\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:449\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[1;32mc:\\Users\\evera\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\evera\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=111.0.5563.147)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x0083DCE3]\n\t(No symbol) [0x007D39D1]\n\t(No symbol) [0x006E4DA8]\n\t(No symbol) [0x006CD0D3]\n\t(No symbol) [0x0072EA8B]\n\t(No symbol) [0x0073D093]\n\t(No symbol) [0x0072ACC6]\n\t(No symbol) [0x00706F68]\n\t(No symbol) [0x007080CD]\n\tGetHandleVerifier [0x00AB3832+2506274]\n\tGetHandleVerifier [0x00AE9794+2727300]\n\tGetHandleVerifier [0x00AEE36C+2746716]\n\tGetHandleVerifier [0x008E6690+617600]\n\t(No symbol) [0x007DC712]\n\t(No symbol) [0x007E1FF8]\n\t(No symbol) [0x007E20DB]\n\t(No symbol) [0x007EC63B]\n\tBaseThreadInitThunk [0x759700F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x771D7BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x771D7B8E+238]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     37\u001b[0m     logging\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(1) FIRST exception for \u001b[39m\u001b[39m{\u001b[39;00msearch_keyword\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00msearch_page\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00msearch_count\u001b[39m}\u001b[39;00m\u001b[39m, retrying...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m     logging\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCurrent URL: \u001b[39m\u001b[39m{\u001b[39;00murl_search\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m     logging\u001b[39m.\u001b[39merror(e)\n\u001b[0;32m     40\u001b[0m     logging\u001b[39m.\u001b[39mexception(\u001b[39m'\u001b[39m\u001b[39mTraceback ->\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url_search' is not defined"
     ]
    }
   ],
   "source": [
    "# create logging file\n",
    "logging = create_logfile()\n",
    "\n",
    "# create daily csv file\n",
    "date = datetime.date.today().strftime('%d-%b-%y')\n",
    "file = f\"output/{date}.csv\"\n",
    "create_file(file, logging)\n",
    "\n",
    "# login to linkedin and assign webdriver to variable\n",
    "wd = login(logging)\n",
    "\n",
    "# URL search terms focusing on what type of skills are required for Data Analyst & Data Scientist\n",
    "search_keywords = ['Data Analyst', 'Data Scientist', 'Data Engineer']\n",
    "    # Titles to remove as search is too long\n",
    "    # ['Business Analyst', 'Operations Analyst', 'Marketing Analyst', 'Product Analyst',\n",
    "    # 'Analytics Consultant', 'Business Intelligence Analyst', 'Quantitative Analyst',  'Data Architect',\n",
    "    # 'Data Engineer', 'Machine Learning Engineer', 'Machine Learning Scientist']\n",
    "search_location = \"Poland\"\n",
    "search_remote = \"false\" # filter for remote positions\n",
    "search_posted = \"\" # filter for past 24 hours\n",
    "\n",
    "# Counting Exceptions\n",
    "exception_first = 0\n",
    "exception_second = 0\n",
    "\n",
    "for search_keyword in search_keywords:\n",
    "    search_keyword = search_keyword.lower().replace(\" \", \"%20\")\n",
    "\n",
    "# Loop through each page and write results to csv\n",
    "    search_page = 0 # start on page 1\n",
    "    search_count = 1 # initiate search count until looks on page\n",
    "    while (search_page < search_count) and (search_page != 1000):\n",
    "        # Search each page and return location after each completion\n",
    "        try:\n",
    "            search_page, search_count, url_search = page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "        except Exception as e:\n",
    "            logging.error(f'(1) FIRST exception for {search_keyword} on {search_page} of {search_count}, retrying...')\n",
    "            logging.error(f'Current URL: {url_search}')\n",
    "            logging.error(e)\n",
    "            logging.exception('Traceback ->')\n",
    "            exception_first += 1\n",
    "            time.sleep(5) \n",
    "            try:\n",
    "                search_page, search_count, url_search = page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "                logging.warning(f'Solved Exception for {search_keyword} on {search_page} of {search_count}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'(2) SECOND exception remains for {search_keyword}. Skipping to next page...')\n",
    "                logging.error(f'Current URL: {url_search}')\n",
    "                logging.error(e)\n",
    "                logging.exception('Traceback ->')\n",
    "                search_page += 25 # skip to next page to avoid entry\n",
    "                exception_second += 1\n",
    "                logging.error(f'Skipping to next page for {search_keyword}, on {search_page} of {search_count}...')\n",
    "\n",
    "# close browser\n",
    "# wd.quit()\n",
    "\n",
    "logging.info(f'LinkedIn data scraping complete with {exception_first} first and {exception_second} second exceptions')\n",
    "logging.info(f'Regard all further alarms...')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a28de7308c275fea1c870545709941aa6f6074775014660680f2515aac085aab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('JobData': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
